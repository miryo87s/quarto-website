---
title: "Small Tricks in Linear Regression"
description: "I am using this post as a memo for some tricks in linear regressions."
abstract: >
  This is a memo of some tricks I found/learned in linear regressions.
date: "2022-10-02"
categories:
  - memo
code-fold: false
---


```{r load_packages, echo=FALSE}
librarian::shelf(sandwich, lmtest, dplyr, quiet = T)
```

# Estimates and SE of Some Sample Mean

Sometimes we are interested in statistics $\theta$ that can be estimated by the sample mean of some function
$$
\hat\theta = \frac1n \sum_i f(W_i)
$$
For example, $\hat\theta$ is the MSE of a predictor $\hat{g}(X_i)$ of $\mathbb{E}[Y_i|X_i]$
$$
\hat\theta = \frac1n \sum_i (Y_i - \hat{g}(X_i))^2
$$
with $f(W_i) = (Y_i - \hat{g}(X_i))^2$.

Calculating $\hat\theta$ is simple but obtaining the standard error of $\hat\theta$ requires extra efforts. In this case, we can utilize the convenient linear regression functions in statistical softwares. The sample mean of some function $f(W_i)$ is equivalent to regressing $f(W_i)$ on a vector of $1$.

```{r mse}

set.seed(1)

n <- 1000
X <- rnorm(n)
Y <- 1 + 2 * X + rnorm(n)

mod <- lm(Y ~ X)
gX <- mod$fitted.values

# MSE by sample mean 
mse <- mean((Y - gX)^2)
cat("The MSE of gX is", mse)

# MSE by linear regression
mse.lm <- lm((Y - gX)^2 ~ 1) |> summary()
cat("The MSE of gX by linear regression is", mse.lm$coef[1], "with SE", mse.lm$coef[2])
```

By this linear regression trick, we can easily obtain $\hat\theta$ and its standard error. The estimator by linear regression is equivalent to the sample mean and is always consistent. However, if there is an estimator in $f(W_i)$ such as $\hat{g}(X_i)$ in the MSE example, to ensure the standard error is correct, we need $f(W_i)$ to satisfy
$$
\mathbb{E}[\partial_\gamma f(W_i)] = 0
$$ {#eq-ortho}
where $\gamma$ is the parameter in $g(X_i)$ with $\hat{g}(X_i) = g(X_i, \hat\gamma)$. This is because we need to take into account the estimation error in $\hat{g}(X_i)$ when calculating the standard error of $\hat\theta$, but this estimation error does not affect $f(W_i)$ if @eq-ortho is satisfied (partial derivative is 0).

In the MSE example, $\hat{g}(X_i) = X_i\hat\gamma$ and $f(W_i) = (Y_i - X_i\gamma)^2$. Then
$$
\mathbb{E}[\partial_\gamma f(W_i)] = \mathbb{E}[-X_i'(Y_i - X_i\gamma)] = 0
$$
by the exogeneity of $X$. So the standard error in the MSE example is correct.

# Multiple Regressions at One Time

Sometimes, we may want to compare coefficients in two different regressions. In this case, a formal hypothesis test is hard to implement because we need a variance-covariance matrix of the coefficients in different regressions. One easy solution is to trick statistical softwares to run two regression at one time by stacking the two regressions.

Suppose we want to compare the coefficients $\alpha$ and $\beta$ in regressions
$$
Y = X\alpha + \epsilon, \quad \quad Z = D\beta + \nu
$$
We can stack the two samples and obtain a long response variable $(Y', Z')'$ and a large design matrix
$$
\begin{pmatrix}
X & 0 \\
0 & D
\end{pmatrix}
$$
so that we have
$$
\begin{pmatrix} Y \\ Z \end{pmatrix} = \begin{pmatrix} X & 0 \\ 0 & D \end{pmatrix} \begin{pmatrix} \alpha \\ \beta \end{pmatrix} + \begin{pmatrix} \epsilon \\ \nu \end{pmatrix}
$$
By running this stacked regression, we are able to obtain the variance-covariance matrix of $\hat\alpha$ and $\hat\beta$, which is essential to conduct hypothesis tests on $\alpha$ and $\beta$.

One advantage of this method is that, using stacked regression, we can recover the OLS estimates of $\alpha$ and $\beta$ in the separate regressions (Seemingly Unrelated Regression can do the same task but cannot always recover OLS estimates). **Note clustered standard error is recommended for the stacked regression** and this method can be extended to IV regressions (or a combination of IV and OLS) by regarding OLS as a special case of IV where the regressors are instrumented by themselves. Below is an example using the dataset of High School and Beyond survey.

```{r stacked}

hsb2 <- foreign::read.dta("https://stats.idre.ucla.edu/stat/stata/notes/hsb2.dta")
hsb2$female <- ifelse(hsb2$female == "female", 1, 0)
hsb2$ses <- as.numeric(hsb2$ses)

# Separate regressions
summary(lm(read ~ female + ses + socst, data = hsb2))$coef[2, 1:2]
summary(lm(math ~ female + ses + science, data = hsb2))$coef[2, 1:2]

# Stacked regression
read.math <- with(hsb2, c(read, math))
X <- cbind(hsb2 |> select("female", "ses", "socst"), matrix(rep(0, nrow(hsb2) * 3), nrow(hsb2), 3))
D <- cbind(matrix(rep(0, nrow(hsb2) * 3), nrow(hsb2), 3), hsb2 |> select("female", "ses", "science"))
df <- data.frame(read.math, rbind(as.matrix(X), as.matrix(D))) |> 
  setNames(c("rm", "female.r", "ses.r", "socst", "female.m", "ses.m", "science"))
df$sid <- c(rep(1, nrow(hsb2)), rep(0, nrow(hsb2)))

stacked <- lm(rm ~ ., data = df)
vcov(stacked, type = "const", cluster = c(rep(1, nrow(hsb2)), rep(0, nrow(hsb2))))[c(2, 5), c(2, 5)]
```






