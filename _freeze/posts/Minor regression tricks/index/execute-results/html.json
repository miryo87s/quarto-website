{
  "hash": "f0c7e297a6f6dcaa780bcc484952067e",
  "result": {
    "markdown": "---\ntitle: \"Small Tricks in Linear Regression\"\ndescription: \"I am using this post as a memo for some tricks in linear regressions.\"\nabstract: >\n  This is a memo of some tricks I found/learned in linear regressions.\ndate: \"2022-10-02\"\ncategories:\n  - memo\ncode-fold: false\n---\n\n::: {.cell}\n\n:::\n\n\n# Estimates and SE of Some Sample Mean\n\nSometimes we are interested in statistics $\\theta$ that can be estimated by the sample mean of some function\n\n$$\n\\hat\\theta = \\frac1n \\sum_i f(W_i)\n$$\n\nFor example, $\\hat\\theta$ is the MSE of a predictor $\\hat{g}(X_i)$ of $\\mathbb{E}[Y_i|X_i]$\n\n$$\n\\hat\\theta = \\frac1n \\sum_i (Y_i - \\hat{g}(X_i))^2\n$$\n\nwith $f(W_i) = (Y_i - \\hat{g}(X_i))^2$.\n\nCalculating $\\hat\\theta$ is simple but obtaining the standard error of $\\hat\\theta$ requires extra efforts. In this case, we can utilize the convenient linear regression functions in statistical softwares. The sample mean of some function $f(W_i)$ is equivalent to regressing $f(W_i)$ on a vector of $1$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\n\nn <- 1000\nX <- rnorm(n)\nY <- 1 + 2 * X + rnorm(n)\n\nmod <- lm(Y ~ X)\ngX <- mod$fitted.values\n\n# MSE by sample mean \nmse <- mean((Y - gX)^2)\ncat(\"The MSE of gX is\", mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE of gX is 1.080436\n```\n:::\n\n```{.r .cell-code}\n# MSE by linear regression\nmse.lm <- lm((Y - gX)^2 ~ 1) |> summary()\ncat(\"The MSE of gX by linear regression is\", mse.lm$coef[1], \"with SE\", mse.lm$coef[2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe MSE of gX by linear regression is 1.080436 with SE 0.04810917\n```\n:::\n:::\n\n\nWe can easily obtain the standard error of the MSE but notice that by doing this, the sum of squared in the MSE is divided by $n$. In small samples, we need to change the denominator to $n - 2$ (lost two degrees of freedom in $\\hat\\beta_0$ and $\\hat\\beta_1$ in $gX$) by multiplying $n/(n - 2)$ and also adjust the standard error accordingly. After adjustment, the MSE is the same as in the anova table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse.lm$coefficients[1] * (n / (n - 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.082601\n```\n:::\n\n```{.r .cell-code}\nanova(mod)['Residuals', 'Mean Sq']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.082601\n```\n:::\n:::\n\n\n# Multiple Regressions at One Time\n\nSometimes, we may want to compare coefficients in two different regressions. In this case, a formal hypothesis test is hard to implement because we need a variance-covariance matrix of the coefficients in different regressions. One easy solution is to trick statistical softwares to run two regression at one time by stacking the two regressions.\n\nSuppose we want to compare the coefficients $\\alpha$ and $\\beta$ in regressions\n\n$$\nY = X\\alpha + \\epsilon, \\quad \\quad Z = D\\beta + \\nu\n$$\n\nWe can stack the two samples and obtain a long response variable $(Y', Z')'$ and a large design matrix\n\n$$\n\\begin{pmatrix}\nX & 0 \\\\\n0 & D\n\\end{pmatrix}\n$$\n\nso that we have\n\n$$\n\\begin{pmatrix} Y \\\\ Z \\end{pmatrix} = \\begin{pmatrix} X & 0 \\\\ 0 & D \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} + \\begin{pmatrix} \\epsilon \\\\ \\nu \\end{pmatrix}\n$$\n\nBy running this stacked regression, we are able to estimate $\\alpha$ and $\\beta$ at the same time and obtain their variance-covariance matrix. **Note clustered standard error is recommended for the stacked regression**. This trick is similar to Seemingly Unrelated Regression (SUR), but the benefit is that this method can be extended to IV regressions (or a combination of IV and OLS) by regarding OLS as a special case of IV where the regressors are instrumented by themselves. Below is an example using the dataset of High School and Beyond survey.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhsb2 <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/stata/notes/hsb2.dta\")\nhsb2$female <- ifelse(hsb2$female == \"female\", 1, 0)\nhsb2$ses <- as.numeric(hsb2$ses)\n\n# Separate regressions\nsummary(lm(read ~ female + ses + socst, data = hsb2))$coef[2, 1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Estimate Std. Error \n -1.511128   1.151079 \n```\n:::\n\n```{.r .cell-code}\nsummary(lm(math ~ female + ses + science, data = hsb2))$coef[2, 1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Estimate Std. Error \n  1.160903   1.041641 \n```\n:::\n\n```{.r .cell-code}\n# Stacked regression\nread.math <- with(hsb2, c(read, math))\nX <- cbind(hsb2 |> select(\"female\", \"ses\", \"socst\"), matrix(rep(0, nrow(hsb2) * 3), nrow(hsb2), 3))\nD <- cbind(matrix(rep(0, nrow(hsb2) * 3), nrow(hsb2), 3), hsb2 |> select(\"female\", \"ses\", \"science\"))\ndf <- data.frame(read.math, rbind(as.matrix(X), as.matrix(D))) |> \n  setNames(c(\"rm\", \"female.r\", \"ses.r\", \"socst\", \"female.m\", \"ses.m\", \"science\"))\ndf$sid <- c(rep(1, nrow(hsb2)), rep(0, nrow(hsb2)))\n\nstacked <- lm(rm ~ ., data = df)\ncoef(stacked)[c(2, 5)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n female.r  female.m \n-1.511128  1.160903 \n```\n:::\n\n```{.r .cell-code}\nvcov(stacked, type = \"const\", cluster = c(rep(1, nrow(hsb2)), rep(0, nrow(hsb2))))[c(2, 5), c(2, 5)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             female.r     female.m\nfemale.r 1.205347e+00 2.701477e-15\nfemale.m 2.701477e-15 1.204577e+00\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}