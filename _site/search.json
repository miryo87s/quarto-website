[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fangzhou Yu",
    "section": "",
    "text": "I am a PhD student in UNSW, Business School under supervision of Scientia Professor Robert Kohn and Associate Professor Seojeong (Jay) Lee.\nMy main research interests include estimation and inference of heterogeneous treatment effect and the application of influence functions in sensitivity analysis. I am also exploring the usefulness of semiparametric influence functions in structural equations."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "2022\nFangzhou Yu. (2022) \"Tests for Heterogeneous Treatment Effects.\"\n        \n        Draft"
  },
  {
    "objectID": "posts/Minor regression tricks/index.html",
    "href": "posts/Minor regression tricks/index.html",
    "title": "Small Tricks in Linear Regression",
    "section": "",
    "text": "Estimates and SE of Some Sample Mean\nSometimes we are interested in statistics \\theta that can be estimated by the sample mean of some function\n\n\\hat\\theta = \\frac1n \\sum_i f(W_i)\n\nFor example, \\hat\\theta is the MSE of a predictor \\hat{g}(X_i) of \\mathbb{E}[Y_i|X_i]\n\n\\hat\\theta = \\frac1n \\sum_i (Y_i - \\hat{g}(X_i))^2\n\nwith f(W_i) = (Y_i - \\hat{g}(X_i))^2.\nCalculating \\hat\\theta is simple but obtaining the standard error of \\hat\\theta requires extra efforts. In this case, we can utilize the convenient linear regression functions in statistical softwares. The sample mean of some function f(W_i) is equivalent to regressing f(W_i) on a vector of 1.\n\nset.seed(1)\n\nn <- 1000\nX <- rnorm(n)\nY <- 1 + 2 * X + rnorm(n)\n\nmod <- lm(Y ~ X)\ngX <- mod$fitted.values\n\n# MSE by sample mean \nmse <- mean((Y - gX)^2)\ncat(\"The MSE of gX is\", mse)\n\nThe MSE of gX is 1.080436\n\n# MSE by linear regression\nmse.lm <- lm((Y - gX)^2 ~ 1) |> summary()\ncat(\"The MSE of gX by linear regression is\", mse.lm$coef[1], \"with SE\", mse.lm$coef[2])\n\nThe MSE of gX by linear regression is 1.080436 with SE 0.04810917\n\n\nBy this linear regression trick, we can easily obtain \\hat\\theta and its standard error. The estimator by linear regression is equivalent to the sample mean and is always consistent. However, if there is an estimator in f(W_i) such as \\hat{g}(X_i) in the MSE example, to ensure the standard error is correct, we need f(W_i) to satisfy\n\n\\mathbb{E}[\\partial_\\gamma f(W_i)] = 0\n\\tag{1}\nwhere \\gamma is the parameter in g(X_i) with \\hat{g}(X_i) = g(X_i, \\hat\\gamma). This is because we need to take into account the estimation error in \\hat{g}(X_i) when calculating the standard error of \\hat\\theta, but this estimation error does not affect f(W_i) if Equation 1 is satisfied (partial derivative is 0).\nIn the MSE example, \\hat{g}(X_i) = X_i\\hat\\gamma and f(W_i) = (Y_i - X_i\\gamma)^2. Then\n\n\\mathbb{E}[\\partial_\\gamma f(W_i)] = \\mathbb{E}[-X_i'(Y_i - X_i\\gamma)] = 0\n\nby the exogeneity of X. So the standard error in the MSE example is correct.\n\n\nRegression across Different Samples\nSometimes, we may want to compare coefficients in two different regressions. In this case, a formal hypothesis test is hard to implement because we need a variance-covariance matrix of the coefficients in different regressions. One easy solution is to trick statistical softwares to run two regression at one time by stacking the two regressions.\nSuppose we want to compare the coefficients \\alpha and \\beta in regressions\n\nY = X\\alpha + \\epsilon, \\quad \\quad Z = D\\beta + \\nu\n\nWe can stack the two samples and obtain a long response variable (Y', Z')' and a large design matrix\n\n\\begin{pmatrix}\nX & 0 \\\\\n0 & D\n\\end{pmatrix}\n\nso that we have\n\n\\begin{pmatrix} Y \\\\ Z \\end{pmatrix} = \\begin{pmatrix} X & 0 \\\\ 0 & D \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} + \\begin{pmatrix} \\epsilon \\\\ \\nu \\end{pmatrix}\n\nBy running this stacked regression, we are able to obtain the variance-covariance matrix of \\hat\\alpha and \\hat\\beta, which is essential to conduct hypothesis tests on \\alpha and \\beta\nOne advantage of this method is that, using stacked regression, we can recover the OLS estimates of \\alpha and \\beta in the separate regressions (but other methods such as Seemingly Unrelated Regression cannot. Note clustered standard error is recommended for the stacked regression if there is overlap between the two datasets"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Blog",
    "section": "",
    "text": "memo\n\n\n\n\nI am using this post as a memo for some tricks in linear regressions.\n\n\n\n\n\n\nOct 2, 2022\n\n\nFangzhou Yu\n\n\n\n\n\n\nNo matching items"
  }
]